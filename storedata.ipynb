{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone for project\n",
    "\n",
    "This Jupyter notebook file contains Python code for cleaning and inserting data from the given csv file into a PostGreSQL database. The csv file in this case is the file containing 1mio. rows. The psycopg2 library is used to establish a connection to the DB and pandas is used as intermediary storage, as it contains a range of practical functions for working with data like this. In terms of tasks, this notebook contains the answer to Tasks 2, 3, and 4, all which can be found below with an explanation of the most important choices. As the overarching goal of the milestone assignment is to show a functional DB with support for the entire FakeNewsCorpus dataset (or at least the first million rows), efficiency both in loading and querying is of high importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import psycopg2, time, re\n",
    "\n",
    "# Make connection to database\n",
    "connection = psycopg2.connect(\n",
    "    user = \"athanar\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"datascience\")\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "For task 2 a DB schema is necessary to properly store the data into the DB. Our choice of schema can be seen in the figure below, as well as the SQL code used to make it in the next cell. As the schema design has been discussed both at lectures, exercise classes as well as the discussion forums, this design should be similar to the commonly available DB schema.\n",
    "\n",
    "![Schema](https://scontent-amt2-1.xx.fbcdn.net/v/t1.15752-9/93259206_2549607885294684_6675379213874233344_n.png?_nc_cat=101&_nc_sid=b96e70&_nc_ohc=02hwRasd1PAAX_2Me4X&_nc_ht=scontent-amt2-1.xx&oh=8a9252e5bb0c8b98c9d6e1583fb28b93&oe=5EC73563)\n",
    "\n",
    "Central to the schema is the Article table. From this, reference tables, with many-to-many relations, tags and writtenby connect to related information. For tags the connected table Keyword contains all the unique meta keywords. Writtenby links articles to their authors. Typelinks functions as a many-to-one relation, where many articles can be of the same unique type in the Type table. Webpage contains the unique URL of an article and connects as a many-to-one relation to the domain of the article, as many articles and urls can come from the same domain. \n",
    "\n",
    "The use of intermediary tables webpage, typelinks, tags and writtenby is reasoned by higher efficiency. As unique IDs, often sequenced, are fast to compare and query, using such tables/relations is superior to querying/comparing the actual values. This is especially the case for the many-to-many relations. \n",
    "\n",
    "By only storing the unique domains, authors, types and keywords, redundant table entries are avoided. By only storing the actual relations in intermediary tables, the DB design becomes tighter. In addition the intermediary tables uses Foreign Keys on the linked IDs ensuring that each row contains a valid relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables_str = \"\"\"\n",
    "    DROP SCHEMA public CASCADE;\n",
    "    CREATE SCHEMA public;\n",
    "    GRANT ALL ON SCHEMA public TO athanar, postgres;\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Article (\n",
    "      articleID INT NOT NULL PRIMARY KEY,\n",
    "      title VARCHAR NULL,\n",
    "      content VARCHAR NULL,\n",
    "      summary VARCHAR NULL,\n",
    "      scrapedAt TIMESTAMP,\n",
    "      insertedAt TIMESTAMP,\n",
    "      updatedAt TIMESTAMP\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Types (\n",
    "      typeID SERIAL PRIMARY KEY,\n",
    "      typeValue VARCHAR NOT NULL UNIQUE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Typelinks (\n",
    "      articleID INT REFERENCES Article(articleID),\n",
    "      typeID INT REFERENCES Types(typeID)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Keyword (\n",
    "      keywordID SERIAL PRIMARY KEY,\n",
    "      keywordValue VARCHAR NOT NULL UNIQUE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Tags (\n",
    "      articleID INT REFERENCES Article(articleID),\n",
    "      keywordID INT REFERENCES Keyword(keywordID)  \n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Domain (\n",
    "      domainID SERIAL PRIMARY KEY,\n",
    "      domainURL VARCHAR NOT NULL UNIQUE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Webpage (\n",
    "      articleID INT REFERENCES Article(articleID),\n",
    "      domainID INT REFERENCES Domain(domainID),\n",
    "      webpageURL VARCHAR NOT NULL UNIQUE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Author (\n",
    "      authorID SERIAL PRIMARY KEY,\n",
    "      authorName VARCHAR NOT NULL UNIQUE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS WrittenBy (\n",
    "      articleID INT REFERENCES Article(articleID),\n",
    "      authorID INT REFERENCES Author(authorID)\n",
    "    );\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "start_time = time.time()\n",
    "reader = pd.read_csv(\n",
    "    \"1mio-raw.csv\", \n",
    "    encoding='utf-8', \n",
    "    chunksize=10000)\n",
    "\n",
    "# Read SQL file\n",
    "def executeScriptFromFile(filename):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "        try:\n",
    "            cursor.execute(command)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "def executeScriptFromString(string):\n",
    "    sqlcommands = string.split(';')\n",
    "    for command in sqlCommands:\n",
    "        try:\n",
    "            cursor.execute(command)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "def clean(s): \n",
    "    if (isinstance(s, float)):\n",
    "        return str(s) \n",
    "    # Replaces url's with < url >\n",
    "    string = re.sub(r'http\\S+', '< url >', s)\n",
    "    \n",
    "    # Replaces integers with <number>\n",
    "    string = re.sub(r'\\b\\d+', ' <number> ', string)\n",
    "    \n",
    "    # Make lowercase and deletes newline\n",
    "    string = s.lower().replace('\\n\\n', '')\n",
    "\n",
    "    # Spaces between acceptable ASCII chars\n",
    "    string = re.sub(r'([\\x21-\\x2f\\x3a-\\x60\\x7b-\\x7e])', r' \\1', string)\n",
    "    \n",
    "    # Removes extended ASCII and unicode chars\n",
    "    string = re.sub(r'[\\u2000-\\u2027\\x80-\\xff]', '', string)\n",
    "    \n",
    "    # Removes subsequent spaces\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    \n",
    "    string = string.strip()\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Task 2\n",
    "\n",
    "Above the script used to load the file can be seen. Pandas csv reader is used to read the file, which is done in chunks to improve performance (otherwise such a large file uses too much memory, especially when processed). Two functions are used to execute SQL commands, where the commands can either be passed as a file or as a string. The SQL string to make the actual tables in the schema can be seen in the cell above. \n",
    "\n",
    "A function for cleaning a string is also present in the cell above. This cleaning is primarily used on the title and content of article, to ensure readable information, as well as removing junk signs from the string. For some of the columns, such as author and keywords, cleaning is also done in the extractParts function below. The reason for this, is that the authors and keywords are stored in a long string and needs to be split into individual elements instead. While the information is already being processed it might as well be cleaned too.\n",
    "\n",
    "Storing of the data into the PostGreSQL DB, makes use of the established connection and insert statements. The function insertTable below is used to mass insert data with the executemany statement, which is significantly faster than inserting individual elements into the table iteratively, as each INSERT statement comes with a large overhead. \n",
    "\n",
    "As the data from file has been read into a pandas dataframe, each colum can be chosen and operated on as seen in the loop, which iterates through all chunks. Each table is handled separately, though insertions happens for all tables in each chunk to ensure efficient insertion. \n",
    "\n",
    "First the data is cleaned and rows without an article ID are removed. Then they are inserted by order of relations. Article first as all other tables reference this, then types, then the links between and so on. In this way correct insertion is assured, as Foreign Key errors happen if a relation table is made before the actual id of the relations exist. \n",
    "\n",
    "To ensure uniqueness of all elements which should be unique, such as authors, keywords and domains, python sets are used. These allow for set operations, so no duplicates can exist and only new elements are inserted into tables. In addition dictionaries are used to maintain many-to-many relations such as authors and keywords, as the list of elements should be extracted from a string and paired with their relational articleID.\n",
    "\n",
    "This process is timed to ensure proper running time, though it is still a slow process for such a large dataset. In addition, it is properly safeguarded with a message if an error occurs, so it can properly continue. This allows for targeted insertion afterwards, if a certain part fails, instead of having to redo the entire DB insertion. This function has successfully inserted the entire 1mio-rows dataset into a PostGreSQL DB.\n",
    "\n",
    "Uncomment to run below, but beware that the SQL script will also drop DB schema as it is right now, so only run it once (necessary for testing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storedata():\n",
    "    # Variables:\n",
    "    new_keywords = []\n",
    "    new_tags = []\n",
    "    new_domains = []\n",
    "    new_authors = []    \n",
    "    new_types = []\n",
    "    \n",
    "    # Inserts dataframe into database\n",
    "    def insertTable(cols, vals, target):\n",
    "        try: \n",
    "            sql = \"INSERT INTO \"+target+\" (\" +cols + \") VALUES (\" + \"%s,\"*(len(vals.iloc[0])-1) + \"%s)\"\n",
    "            cursor.executemany(sql, vals.values.tolist())\n",
    "            connection.commit()\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong with: %s %s\" % (target, str(e)))\n",
    "\n",
    "    # Get typeid for type string\n",
    "    def typeLookup(typeval):\n",
    "        if (isinstance(typeval, float)):\n",
    "            return 12\n",
    "        else:\n",
    "            return new_types.index(typeval) + 1\n",
    "\n",
    "    # Extract comma separated parts of string column\n",
    "    def extractParts(ids, column):\n",
    "        tmp_dict = {}\n",
    "        tmp = []\n",
    "        for i in range(len(column)):\n",
    "            if (isinstance(column.iloc[i], float)):\n",
    "                tmp.extend(str(column.iloc[i]))\n",
    "                tmp_dict[ids.iloc[i]] =  str(column.iloc[i])\n",
    "            elif (column.iloc[i] == \"[\\'\\']\"):\n",
    "                continue\n",
    "            else:\n",
    "                new_vals = (column.iloc[i]\n",
    "                            .replace('[', '')\n",
    "                            .replace(']', '')\n",
    "                            .replace('\\'', '')\n",
    "                            .replace('\\\"', '')\n",
    "                            .lower()\n",
    "                            .split(', '))\n",
    "                tmp.extend(new_vals)\n",
    "                tmp_dict[ids.iloc[i]] = new_vals\n",
    "        return set(tmp), tmp_dict\n",
    "    \n",
    "    i = 1\n",
    "    for data in reader:\n",
    "        # Size; Highly temporary for testing purposes. Can be adjusted to test smaller dataset\n",
    "        if (i > 500):\n",
    "            break\n",
    "        try:\n",
    "            # Clean data\n",
    "            data['id'] = pd.to_numeric(data['id'], errors='coerce')\n",
    "            data = data[data['id'].notna()]\n",
    "            data['content'] = data['content'].apply(clean)\n",
    "            data['title'] = data['title'].apply(clean)\n",
    "            data['summary'] = data['summary'].apply(clean)\n",
    "        except Exception as e:\n",
    "            print(\"Cleaning went wrong in round: %s %s\" % (i, str(e)))\n",
    "        try:\n",
    "            # Fetches article from dataframe\n",
    "            article = data.iloc[:,[1,9,5,15,6,7,8]]\n",
    "            articleval = \"articleID, title, content, summary, scrapedAt, insertedAt, updatedAt\"\n",
    "            insertTable(articleval, article, \"Article\")\n",
    "        except Exception as e:\n",
    "            print(\"Article insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "            \n",
    "        # Fetches types from dataframe, done like this as the first round \n",
    "        # finds all relevant types \n",
    "        if (len(new_types) < 1):\n",
    "            types = data['type'].drop_duplicates().dropna()\n",
    "            typeframe = pd.DataFrame(types).rename(columns={'type':'typeValue'})\n",
    "            new_types = list(types)\n",
    "            insertTable(\"typeValue\", pd.DataFrame(types), \"Types\") \n",
    "            \n",
    "        try:\n",
    "            # Fills Typelinks\n",
    "            articleid = data.iloc[:,[1]]\n",
    "            typeid = data['type'].apply(typeLookup)\n",
    "            typelinks = pd.concat([articleid, typeid], axis=1, ignore_index=True)\n",
    "            insertTable(\"articleID, typeID\", typelinks, \"Typelinks\")\n",
    "        except Exception as e:\n",
    "            print(\"Typelinks insertion went wrong in round:%s %s\" % (i, str(e)))\n",
    "        \n",
    "        try: \n",
    "            # Fetches keywords from dataframe and inserts new keywords\n",
    "            keywords, keyword_dict = extractParts(data['id'],data['meta_keywords'])\n",
    "            keyword_list = list(keywords.difference(set(new_keywords)))\n",
    "            new_keywords.extend(keyword_list)\n",
    "            if (len(keyword_list) > 0):\n",
    "                insertTable(\"keywordValue\", pd.DataFrame(keyword_list), \"Keyword\")\n",
    "        except Exception as e:\n",
    "            print(\"Keywords insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        try: \n",
    "            # Inserts keywordlinks\n",
    "            tmp_kw = []\n",
    "            for k, v in keyword_dict.items():\n",
    "                for kword in v:\n",
    "                    tmp_kw.append([k, new_keywords.index(kword)+1])\n",
    "            insertTable(\"articleID, keywordID\", pd.DataFrame(tmp_kw), \"KeywordLinks\")\n",
    "        except Exception as e:\n",
    "            print(\"Tags insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "            \n",
    "        try: \n",
    "            # Fetches tags from dataframe and inserts new tags\n",
    "            tags, tag_dict = extractParts(data['id'],data['tags'])\n",
    "            tag_list = list(tags.difference(set(new_tags)))\n",
    "            new_tags.extend(tag_list)\n",
    "            if (len(tag_list) > 0):\n",
    "                insertTable(\"tagValue\", pd.DataFrame(tag_list), \"Tag\")\n",
    "        except Exception as e:\n",
    "            print(\"Keywords insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        try: \n",
    "            # Inserts taglinks\n",
    "            tmp_tag = []\n",
    "            for t, v in tag_dict.items():\n",
    "                for ts in v:\n",
    "                    tmp_tag.append([t, new_tags.index(ts)+1])\n",
    "            insertTable(\"articleID, tagID\", pd.DataFrame(tmp_tag), \"TagLinks\")\n",
    "        except Exception as e:\n",
    "            print(\"Tags insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        try:\n",
    "            # Fetches domain from dataframe and inserts new domains\n",
    "            domain = set(data.loc[:,'domain'])\n",
    "            domain_list = list(domain.difference(set(new_domains)))\n",
    "            new_domains.extend(domain_list)\n",
    "            if (len(domain_list) > 0):\n",
    "                insertTable(\"domainURL\", pd.DataFrame(domain_list), \"Domain\")\n",
    "        except Exception as e:\n",
    "            print(\"Domain insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        try:\n",
    "            # Fetches webpageURL from dataframe and inserts\n",
    "            new_webs = data['domain'].apply(lambda x: new_domains.index(x)+1)\n",
    "            dom_frame = pd.DataFrame(\n",
    "                {'id': data['id'], 'domain': new_webs, 'url': data['url']})\n",
    "            insertTable(\"articleID, domainID, webpageurl\", dom_frame, \"Webpage\")\n",
    "        except Exception as e:\n",
    "            print(\"Webpage insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        try:\n",
    "            # Fetches authors from dataframe and inserts new authors\n",
    "            authors, authors_dict = extractParts(data['id'], data['authors'])\n",
    "            author_list = list(authors.difference(set(new_authors)))\n",
    "            new_authors.extend(author_list)\n",
    "            if (len(author_list) > 0):\n",
    "                insertTable(\"authorName\", pd.DataFrame(author_list), \"Author\")\n",
    "        except Exception as e:\n",
    "            print(\"Authors insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        try:\n",
    "            # Inserts into writtenby\n",
    "            tmp_aut = []\n",
    "            for k, v in authors_dict.items():\n",
    "                for kword in v:\n",
    "                    tmp_aut.append([k, new_authors.index(kword)+1])\n",
    "            insertTable(\"articleID, authorID\", pd.DataFrame(tmp_aut), \"WrittenBy\")\n",
    "        except Exception as e:\n",
    "            print(\"Writtenby insertion went wrong in round: %s %s\" % (i, str(e)))\n",
    "        \n",
    "        # Round counter for timing\n",
    "        print(\"Round %d took %s seconds\" % (i,time.time() - start_time))\n",
    "        i = i+1\n",
    "\n",
    "    print(\"Finished. Took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 took 45.923566579818726 seconds\n",
      "Round 2 took 87.02437114715576 seconds\n",
      "Round 3 took 133.10276794433594 seconds\n",
      "Round 4 took 236.31031560897827 seconds\n",
      "Round 5 took 274.57006192207336 seconds\n",
      "Round 6 took 313.32639956474304 seconds\n",
      "Round 7 took 376.4915282726288 seconds\n",
      "Round 8 took 474.39187836647034 seconds\n",
      "Round 9 took 524.0554986000061 seconds\n",
      "Round 10 took 571.4146285057068 seconds\n",
      "Round 11 took 627.0398869514465 seconds\n",
      "Round 12 took 676.7929012775421 seconds\n",
      "Round 13 took 717.9069180488586 seconds\n",
      "Round 14 took 823.6844954490662 seconds\n",
      "Round 15 took 878.8675203323364 seconds\n",
      "Round 16 took 961.156378030777 seconds\n",
      "Round 17 took 1054.3395442962646 seconds\n",
      "Round 18 took 1127.065241098404 seconds\n",
      "Round 19 took 1230.3167827129364 seconds\n",
      "Round 20 took 1265.990296125412 seconds\n",
      "Round 21 took 1301.758017539978 seconds\n",
      "Round 22 took 1339.954978942871 seconds\n",
      "Round 23 took 1378.9899702072144 seconds\n",
      "Round 24 took 1433.87468791008 seconds\n",
      "Round 25 took 1535.4908590316772 seconds\n",
      "Round 26 took 1721.9858541488647 seconds\n",
      "Round 27 took 1982.4932479858398 seconds\n",
      "Round 28 took 2072.2790105342865 seconds\n",
      "Round 29 took 2152.193500995636 seconds\n",
      "Round 30 took 2228.12459731102 seconds\n",
      "Round 31 took 2307.0026173591614 seconds\n",
      "Round 32 took 2407.913283586502 seconds\n",
      "Round 33 took 2499.4339241981506 seconds\n",
      "Round 34 took 2686.8819286823273 seconds\n",
      "Round 35 took 3055.3453226089478 seconds\n",
      "Round 36 took 3404.0607578754425 seconds\n",
      "Round 37 took 3575.5181860923767 seconds\n",
      "Round 38 took 3722.349216938019 seconds\n",
      "Round 39 took 3785.7331681251526 seconds\n",
      "Round 40 took 3837.947681427002 seconds\n",
      "Round 41 took 3939.286164045334 seconds\n",
      "Round 42 took 4231.34671497345 seconds\n",
      "Round 43 took 4320.839720010757 seconds\n",
      "Round 44 took 4387.313933610916 seconds\n",
      "Round 45 took 4544.381489753723 seconds\n",
      "Round 46 took 4660.373037099838 seconds\n",
      "Round 47 took 4748.697342634201 seconds\n",
      "Round 48 took 4789.895738124847 seconds\n",
      "Round 49 took 4855.441407442093 seconds\n",
      "Round 50 took 5034.952311515808 seconds\n",
      "Round 51 took 5228.859991550446 seconds\n",
      "Round 52 took 5338.009750366211 seconds\n",
      "Round 53 took 5404.509922742844 seconds\n",
      "Round 54 took 5450.528623819351 seconds\n",
      "Round 55 took 5528.005426168442 seconds\n",
      "Round 56 took 5633.45773935318 seconds\n",
      "Round 57 took 5763.060103416443 seconds\n",
      "Round 58 took 6268.616477012634 seconds\n",
      "Round 59 took 6693.0446898937225 seconds\n",
      "Round 60 took 6780.660012245178 seconds\n",
      "Round 61 took 6950.590945482254 seconds\n",
      "Round 62 took 7023.388659238815 seconds\n",
      "Round 63 took 7151.60391831398 seconds\n",
      "Round 64 took 7197.636751174927 seconds\n",
      "Round 65 took 7243.9145402908325 seconds\n",
      "Round 66 took 7334.870063781738 seconds\n",
      "Round 67 took 7431.496853590012 seconds\n",
      "Round 68 took 7464.1305413246155 seconds\n",
      "Round 69 took 7509.869575023651 seconds\n",
      "Round 70 took 7595.679217100143 seconds\n",
      "Round 71 took 7916.6785316467285 seconds\n",
      "Round 72 took 8698.887815237045 seconds\n",
      "Round 73 took 8761.494757175446 seconds\n",
      "Round 74 took 8868.104653835297 seconds\n",
      "Round 75 took 9017.481797456741 seconds\n",
      "Round 76 took 9300.383489131927 seconds\n",
      "Round 77 took 9464.55860543251 seconds\n",
      "Round 78 took 9535.289998054504 seconds\n",
      "Round 79 took 9809.691231489182 seconds\n",
      "Round 80 took 10129.689946889877 seconds\n",
      "Round 81 took 10338.344325304031 seconds\n",
      "Round 82 took 10380.571432828903 seconds\n",
      "Round 83 took 10602.264302253723 seconds\n",
      "Round 84 took 10697.607998132706 seconds\n",
      "Round 85 took 10880.014642953873 seconds\n",
      "Round 86 took 10939.03643488884 seconds\n",
      "Round 87 took 11253.236958265305 seconds\n",
      "Round 88 took 11554.642482042313 seconds\n",
      "Round 89 took 11743.451137781143 seconds\n",
      "Round 90 took 11924.432030916214 seconds\n",
      "Round 91 took 12303.081733226776 seconds\n",
      "Round 92 took 12571.089948415756 seconds\n",
      "Round 93 took 12777.700073719025 seconds\n",
      "Round 94 took 13071.99618268013 seconds\n",
      "Round 95 took 13204.316071510315 seconds\n",
      "Round 96 took 13469.507392406464 seconds\n",
      "Round 97 took 13741.935421705246 seconds\n",
      "Round 98 took 14037.292882680893 seconds\n",
      "Round 99 took 14339.639548063278 seconds\n",
      "Round 100 took 14464.064521551132 seconds\n",
      "Finished. Took 14464.064521551132 seconds\n"
     ]
    }
   ],
   "source": [
    "executeScriptFromFile('create_tables.sql')\n",
    "storedata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 and Task 4\n",
    "\n",
    "Below are the functions used for Task 3 and Task 4, where the execute function are highly similar to the one used below, with prints and fetches as the difference. The queries themselves can be seen as well, where they are written as strings. Currently the exact same SQL commands are stored in a file, so they can be run at once and run in SQL command line directly too. But if there's an interest in running the functions with the string, the function and the strings are present.\n",
    "\n",
    "The reliable domains are found by a waterfall of substrings, which significantly improves the efficiency relative to comparing the full tables to eachother. Prolific authors can be found in a simpler way where the elements are simply selected by crossreferencing the tables. While this could be improved with subqueries as well, we have found that it is almost instant as is, and thus no reason to significantly alter it.\n",
    "\n",
    "The keywords are counted by making an inner join of a small part of the tags dataset on itself. It then compares all elements with all elements which is not the same as itself and gets the ones where the keyword is the same. Then the number of rows is counted, and divided by two, as the elements are paired both ways. As this is an expensive query, it is done on a small subset, but the number can be changed to any other number if there's an interest in seeing more common pairs.\n",
    "\n",
    "For the exploration in task 4, four areas were examined. First we looked at domains and how many articles were written on each of each type. The output of this is interesting because it gives insight into which types are most frequently written, where the quality as expected is classified as low (Types unreliable, fake, political and conspiracy). \n",
    "\n",
    "Next the amount of articles written of each type is counted, which provides a simple overview of the distribution of articles. Then the amount of articles written by each author is found and finally the amount of articles of each type is found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryfile(filename):\n",
    "    # Read SQL file\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "        try:\n",
    "            cursor.execute(command)\n",
    "            print(\"\\n-----------------------------------Query---------------------------------------\\n\")\n",
    "            print(pd.DataFrame(cursor.fetchall()))\n",
    "        except:\n",
    "            continue \n",
    "def querystr(string):\n",
    "    sqlCommands = string.split(';')\n",
    "    for command in sqlCommands:\n",
    "        try:\n",
    "            cursor.execute(string)\n",
    "            print(\"\\n-----------------------------------Query---------------------------------------\\n\")\n",
    "            print(pd.DataFrame(cursor.fetchall()))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliable domains of news articles for Task 3.1\n",
    "reliable_domains = \"\"\"\n",
    "    SELECT domainURL\n",
    "    FROM Domain\n",
    "    WHERE domainID in (\n",
    "        SELECT domainID\n",
    "        FROM Webpage\n",
    "        Where articleID in (\n",
    "            SELECT articleID \n",
    "            FROM typelinks\n",
    "            WHERE typeID = 8 \n",
    "            AND articleID in (\n",
    "                SELECT articleID\n",
    "                FROM Article\n",
    "                WHERE scrapedAt >= to_timestamp(2018-1-15)\n",
    "            )\n",
    "        )\n",
    "    );\"\"\"\n",
    "\n",
    " \n",
    "# Most prolific authors of fake type news articles for Task 3.2\n",
    "prolific_fake_authors = \"\"\"\n",
    "    SELECT authorName, COUNT(w.authorid) \n",
    "    FROM writtenby w, typelinks t, author a \n",
    "    WHERE w.articleid = t.articleid \n",
    "        AND t.typeid = 7 \n",
    "        AND w.authorID = a.authorID\n",
    "    GROUP BY a.authorName, w.authorid \n",
    "    ORDER BY count DESC\n",
    "    LIMIT 20; \"\"\"\n",
    "\n",
    "# Pairs of article ids with the exact same set of keywords for Task 3.3\n",
    "count_keywordpairs = \"\"\"\n",
    "    WITH tags_small AS (SELECT * from tags where articleID <= 500),\n",
    "         article_small AS (SELECT DISTINCT articleID FROM tags_small)\n",
    "    SELECT COUNT(DISTINCT t1.articleID) / 2\n",
    "    FROM tags_small t1\n",
    "    INNER JOIN tags_small t2 \n",
    "    ON t1.articleID <> t2.articleID AND t1.keywordID = t2.keywordID;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_articles_domain = \"\"\"\n",
    "    SELECT domainURL, typeValue, COUNT(w.domainID)\n",
    "    FROM Webpage w, Domain d, typelinks tl, Types t\n",
    "    WHERE w.domainID in (\n",
    "        SELECT domainID\n",
    "        FROM Webpage\n",
    "        Where articleID in (\n",
    "            SELECT articleID \n",
    "            FROM typelinks\n",
    "            WHERE typeID = 2 \n",
    "            )\n",
    "        ) \n",
    "    AND w.domainID = d.domainID\n",
    "    AND w.articleID = tl.articleID\n",
    "    AND tl.typeID = t.typeID\n",
    "    GROUP BY d.domainURL, typeValue\n",
    "    ORDER BY count DESC;\"\"\"\n",
    "\n",
    "articles_per_type = \"\"\"\n",
    "    SELECT typeValue, COUNT(articleID)\n",
    "    FROM Types t, typelinks tl\n",
    "    WHERE t.typeID = tl.typeID\n",
    "    GROUP BY typeValue\n",
    "    ORDER BY count DESC;\"\"\"\n",
    "\n",
    "articles_per_author = \"\"\"\n",
    "    SELECT authorName, COUNT(w.articleID)\n",
    "    FROM Writtenby w, Author a \n",
    "    WHERE w.authorID = a.authorID\n",
    "    GROUP BY authorName\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 100;\"\"\"\n",
    "\n",
    "article_type_count_author = \"\"\"\n",
    "    SELECT authorName, typeValue, COUNT(a.authorID)\n",
    "    FROM Author a, Writtenby w, typelinks tl, Types t\n",
    "    WHERE w.articleID = tl.articleID AND a.authorID = w.authorID AND tl.typeID = t.typeID\n",
    "    GROUP BY authorName, typeValue\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 100;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryfile('queries.sql')\n",
    "#queryfile('queries2.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "The webscraping algorithm has been created with the help of pandas, beautifulsoup and urllib. Instead of starting the iteration from A, the specific input has been set to start iterating from articles beginning with J. This has been done to dramatically decrease running time. The idea is to start the iteration from an arbitrary URL and then search through the headers of the HTML code to see whether or not the desired articles have been found. Although this specific code handles the URL of articles starting with J, slight modifications could have been made for the algorithm to start all the way from A-Z and only selecting articles in question. It then creates a list of article URL's   where url requests are made and information of importance is fetched and stored in a pandas dataframe.\n",
    "\n",
    "A simple text cleaning has also been implemented for the only purpose to clean the html code - discarding all unwanted data. \n",
    "The pandas dataframe returned by the algorithm consists of 3 columns; title, date and content. The title and date has specifically been found using the functionality of the beautifulsoup package, which makes it possible to iterate through the HTML code more freely. Out of the approximately 7500 articles present in Wikinews around 3000 (from the range J-T) is stored and returned in the pandas dataframe.\n",
    "One of the challenging asepcts of doing this exercise was to get an understanding of the HTML code of a website, what we wanted to extract, and what felt less important. This felt very different from what we are used to seeing, but nevertheless a heirarcically structure is present and ultimately this helped us to get a better understanding of how webpages are constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def webscraping_cleanup(string):\n",
    "    retval = re.sub(r'<.+?>', '', string)\n",
    "    retval = re.sub(r'\\b.+\\xa0', '', retval).replace('\\n, ', '').lower()\n",
    "    retval = re.sub(r'\\n\\n\\n\\n\\n.+', '', retval).replace('\\n','')\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "# Start letters of articles to find: [J-T]\n",
    "\n",
    "def webscraping(first_article, end_article, URL):\n",
    "    column_names = ['title', 'date', 'content']\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    condition = True\n",
    "    while condition:\n",
    "        source = urllib.request.urlopen(\"https://en.wikinews.org\" + URL).read()\n",
    "        soup = bs.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        all_articles = soup.find_all('div', attrs={'id':'mw-pages'})\n",
    "\n",
    "        next_list = []\n",
    "        a_list = []\n",
    "        headers = []\n",
    "        for a in all_articles:\n",
    "            # Finds html code for next and previous page (although only next is used)    \n",
    "            next_list.extend(a.find_all(title='Category:Politics and conflicts'))\n",
    "            # Finds information about all articles starting with [J-T]\n",
    "            a_list.extend(a.find_all(title=re.compile(r'^['+first_article+'-'+end_article+']\\w')))\n",
    "            # Finds headers of all_articles\n",
    "            headers.extend(a.find_all(re.compile(r'^h[1-6]$')))\n",
    "        \n",
    "        # Find last element of headers\n",
    "        if headers[-1].text >= first_article and headers[-1].text <= end_article:\n",
    "            condition = True\n",
    "            print('Processing articles: ',headers[-1].text)\n",
    "        else: \n",
    "            condition = False\n",
    "\n",
    "        # Finds URL of next source to be iterated through    \n",
    "        URL = next_list[-1]['href']\n",
    "        for url in a_list:\n",
    "            # Makes urlrequest to all articles found in the desired range: [J-T]\n",
    "            article_source = urllib.request.urlopen(\"https://en.wikinews.org\" + url['href']).read()\n",
    "            article_soup = bs.BeautifulSoup(article_source, 'lxml')\n",
    "    \n",
    "            article_title = article_soup.head.title.text.replace(' - Wikinews, the free news source', '')\n",
    "    \n",
    "            # Some publishDates appear as None in html\n",
    "            try:\n",
    "                article_date = article_soup.find(id='publishDate')['title']\n",
    "            except:\n",
    "                article_date = 'unknown'\n",
    "            \n",
    "            article_text = article_soup.find_all('div', attrs={'class':'mw-parser-output'})\n",
    "            p_list = []\n",
    "            for p in article_text:\n",
    "                #TODO: Find text from article other than that located in <p>\n",
    "                p_list.extend(p.find_all('p'))\n",
    "    \n",
    "            # Simple text cleanup\n",
    "            article_text = webscraping_cleanup(str(p_list))\n",
    "            \n",
    "            df = df.append({'title':article_title, 'date':article_date, 'content':article_text}, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = webscraping('J', 'T', '/w/index.php?title=Category:Politics_and_conflicts&pageuntil=Jesse+Jackson+apologizes+for+comment+about+Barack+Obama#mw-pages')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
